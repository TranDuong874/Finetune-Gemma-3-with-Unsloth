model:
  model_name: "unsloth/gemma-3-1b-it"
  output_dir: "results"
  max_length: 512
  packing: false
  num_train_epochs: 5
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4          
  gradient_checkpointing: false
  optim: "adamw_8bit"
  warmup_steps: 5                          
  logging_steps: 1
  save_strategy: "epoch"
  eval_strategy: "epoch"
  learning_rate: 5e-5
  weight_decay: 0.01                       
  lr_scheduler_type: "constant"
  seed: 3407                               
  report_to: "none"                        
  load_in_8bit: false
  load_in_4bit: false
  max_steps: 30                             

peft_config:
  r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  bias: null
  random_state: 42
  use_rslora: false
  finetune_mlp_modules: true
  target_modules: 
    - "gate_proj"
    - "up_proj"
    - "down_proj"

dataset:
  train_path: 'data/train.json'
  valid_path: 'data/valid.json'
  test_path: 'data/test.json'

hp_search:
  enable: true  # Set to false to disable hyperparameter search
  n_trials: 20
  hp_space:
    # Learning rate optimization
    learning_rate:
      type: "loguniform"
      low: 1e-6
      high: 1e-3
    
    # Weight decay optimization
    weight_decay:
      type: "loguniform" 
      low: 1e-4
      high: 1e-1
    
    # LoRA rank optimization
    r:
      type: "int"
      low: 4
      high: 64
      
    # LoRA alpha optimization
    lora_alpha:
      type: "int"
      low: 8
      high: 128
      
    # LoRA dropout optimization
    lora_dropout:
      type: "uniform"
      low: 0.0
      high: 0.3
      
    # Batch size optimization
    per_device_train_batch_size:
      type: "int"
      low: 1
      high: 8
      
    # Gradient accumulation steps
    gradient_accumulation_steps:
      type: "int"
      low: 1
      high: 16
      
    # Warmup steps optimization
    warmup_steps:
      type: "int"
      low: 0
      high: 100
      
    # Number of epochs optimization
    num_train_epochs:
      type: "int"
      low: 1
      high: 3
      
    # Optimizer choice
    optim:
      type: "categorical"
      choices: 
        - "adamw_8bit"
        - "adamw_torch"
        - "adamw_hf"
        - "sgd"
        
    # Learning rate scheduler
    lr_scheduler_type:
      type: "categorical"
      choices:
        - "linear"
        - "cosine"
        - "constant"
        - "constant_with_warmup"
        - "cosine_with_restarts"